{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% codecell\n",
    "import numpy as np\n",
    "import Plot as plot\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "import scipy.spatial\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib as plt\n",
    "import logging\n",
    "from scipy import signal\n",
    "import learningAlgs as classImportLA\n",
    "import dataManipulation as dataMan\n",
    "from itertools import permutations\n",
    "import importlib\n",
    "from datetime import timedelta\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.cluster import KMeans\n",
    "import timeIntervalPlotter as intervalPlotter\n",
    "import pysal\n",
    "import warnings\n",
    "import boundaryFull_SS_WeightedLumping as WLumping\n",
    "from importlib import reload\n",
    "from scipy.stats import rayleigh\n",
    "import dataManipulation as dataMan\n",
    "import matplotlib.pyplot as plt\n",
    "import processData as processData\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the address of the collected data files (not alligned files or CSV files): /home/sepehr/thesis/data/500f80271400/\n",
      "['500f80271400.txt']\n",
      "here\n",
      "500f80271400.txt is in csvChecker\n",
      "\n",
      "we have the csv file: pulling out data\n",
      "\n",
      "   col1                time  CU\n",
      "0     0 2018-11-13 02:01:32  45\n",
      "1     1 2018-11-13 02:01:38  45\n",
      "2     2 2018-11-13 02:01:44  51\n",
      "3     3 2018-11-13 02:01:50  53\n",
      "4     4 2018-11-13 02:01:56  45\n",
      "now we have the processed data from pandas\n",
      "hello\n",
      "            col1                time   CU    CU/255\n",
      "0              0 2018-11-13 02:01:32   45  0.176471\n",
      "1              1 2018-11-13 02:01:38   45  0.176471\n",
      "2              2 2018-11-13 02:01:44   51  0.200000\n",
      "3              3 2018-11-13 02:01:50   53  0.207843\n",
      "4              4 2018-11-13 02:01:56   45  0.176471\n",
      "5              5 2018-11-13 02:02:02   45  0.176471\n",
      "6              6 2018-11-13 02:02:08   45  0.176471\n",
      "7              7 2018-11-13 02:02:14   48  0.188235\n",
      "8              8 2018-11-13 02:02:20   58  0.227451\n",
      "9              9 2018-11-13 02:02:26   53  0.207843\n",
      "10            10 2018-11-13 02:02:32   45  0.176471\n",
      "11            11 2018-11-13 02:02:38   45  0.176471\n",
      "12            12 2018-11-13 02:02:44   48  0.188235\n",
      "13            13 2018-11-13 02:02:50   45  0.176471\n",
      "14            14 2018-11-13 02:02:56   45  0.176471\n",
      "15            15 2018-11-13 02:03:02   45  0.176471\n",
      "16            16 2018-11-13 02:03:08   48  0.188235\n",
      "17            17 2018-11-13 02:03:14   45  0.176471\n",
      "18            18 2018-11-13 02:03:20   48  0.188235\n",
      "19            19 2018-11-13 02:03:26   51  0.200000\n",
      "20            20 2018-11-13 02:03:32   56  0.219608\n",
      "21            21 2018-11-13 02:03:38   51  0.200000\n",
      "22            22 2018-11-13 02:03:44   45  0.176471\n",
      "23            23 2018-11-13 02:03:50   45  0.176471\n",
      "24            24 2018-11-13 02:03:56   45  0.176471\n",
      "25            25 2018-11-13 02:04:02   45  0.176471\n",
      "26            26 2018-11-13 02:04:14   91  0.356863\n",
      "27            27 2018-11-13 02:04:20   43  0.168627\n",
      "28            28 2018-11-13 02:04:26   53  0.207843\n",
      "29            29 2018-11-13 02:04:32   66  0.258824\n",
      "...          ...                 ...  ...       ...\n",
      "1562706  1562706 2019-08-06 17:31:38   43  0.168627\n",
      "1562707  1562707 2019-08-06 17:31:44   53  0.207843\n",
      "1562708  1562708 2019-08-06 17:31:50   33  0.129412\n",
      "1562709  1562709 2019-08-06 17:31:56   56  0.219608\n",
      "1562710  1562710 2019-08-06 17:32:02   40  0.156863\n",
      "1562711  1562711 2019-08-06 17:32:08   38  0.149020\n",
      "1562712  1562712 2019-08-06 17:32:14   28  0.109804\n",
      "1562713  1562713 2019-08-06 17:32:20   56  0.219608\n",
      "1562714  1562714 2019-08-06 17:32:26   38  0.149020\n",
      "1562715  1562715 2019-08-06 17:32:32  153  0.600000\n",
      "1562716  1562716 2019-08-06 17:32:38   45  0.176471\n",
      "1562717  1562717 2019-08-06 17:32:44   53  0.207843\n",
      "1562718  1562718 2019-08-06 17:32:50   45  0.176471\n",
      "1562719  1562719 2019-08-06 17:32:56   43  0.168627\n",
      "1562720  1562720 2019-08-06 17:33:02   40  0.156863\n",
      "1562721  1562721 2019-08-06 17:33:08   45  0.176471\n",
      "1562722  1562722 2019-08-06 17:33:14   43  0.168627\n",
      "1562723  1562723 2019-08-06 17:33:20   43  0.168627\n",
      "1562724  1562724 2019-08-06 17:33:26   53  0.207843\n",
      "1562725  1562725 2019-08-06 17:33:32   51  0.200000\n",
      "1562726  1562726 2019-08-06 17:33:38   48  0.188235\n",
      "1562727  1562727 2019-08-06 17:33:44   53  0.207843\n",
      "1562728  1562728 2019-08-06 17:33:50   38  0.149020\n",
      "1562729  1562729 2019-08-06 17:33:56   38  0.149020\n",
      "1562730  1562730 2019-08-06 17:34:02   51  0.200000\n",
      "1562731  1562731 2019-08-06 17:34:08   51  0.200000\n",
      "1562732  1562732 2019-08-06 17:34:14   40  0.156863\n",
      "1562733  1562733 2019-08-06 17:34:20   40  0.156863\n",
      "1562734  1562734 2019-08-06 17:34:26   51  0.200000\n",
      "1562735  1562735 2019-08-06 17:34:32   48  0.188235\n",
      "\n",
      "[1562736 rows x 4 columns]\n",
      "please enter how long would be the chunk minutes? 30\n",
      "removing weekends from the data\n",
      "[[0.34180791 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# <codecell>\n",
    "\n",
    "#*******************************************************************************\n",
    "# %%codecell\n",
    "reload(classImportLA)\n",
    "dataFrame = processData.processingData()\n",
    "#address is: /home/netlab/Desktop/thesis/data/1node1-3-5/\n",
    "#/home/netlab/Desktop/thesis/data/500f80271400/\n",
    "data = dataFrame.copy() #copying the dataFrame to have a copy of not edited data\n",
    "\n",
    "print(\"hello\")\n",
    "# <codecell>\n",
    "\n",
    "#*******************************************************************************\n",
    "# %% codecell\n",
    "print(data)\n",
    "data = processData.dataFrameManipulation(data)\n",
    "numberOfStates = 255\n",
    "cuTrans = processData.markovianTransitionMatrixDegree1(data, numberOfStates, \"CU\")\n",
    "normalizedCuTrans = processData.normalizingTransMatrix(cuTrans)\n",
    "cuTrans_cpy = normalizedCuTrans.copy()\n",
    "\n",
    "arrExtra = [0]\n",
    "def particling(x):\n",
    "    if (x[\"col1\"] - 1 not in data[\"col1\"]) or (x[\"col1\"] - 1 < 0):\n",
    "        arrExtra[0] = x[\"time\"]\n",
    "        return 0\n",
    "\n",
    "    elif x[\"timeIndex\"] != data[\"timeIndex\"][x[\"col1\"] - 1]:\n",
    "        arrExtra[0] = x[\"time\"]\n",
    "        return 0\n",
    "\n",
    "    else:\n",
    "        return ((x[\"time\"] - arrExtra[0]).seconds)/6\n",
    "\n",
    "data[\"periodParticle\"] = data.apply(lambda x: particling(x), axis = 1) #numbering each 6 seconds in the dataFrame\n",
    "\n",
    "# print(cuTrans_cpy[0])\n",
    "# <codecell>\n",
    "#*******************************************************************************\n",
    "# data.tail()\n",
    "# plotDataFrame = data.loc[(data[\"timeIndex\"] > 2) & (data[\"timeIndex\"] < 12)]\n",
    "# time = [j for j in range(data_CU_to_numpy.shape[0])]\n",
    "# data_day_to_list = plotDataFrame[\"time\"].to_list()\n",
    "# a = []\n",
    "# a.append(go.Scatter(x = time[:150000], y = data_CU_to_numpy[:150000]))\n",
    "# fileName =\"non-busy hour1-6pm.html\"\n",
    "# offline.plot(a, filename=fileName, image='svg')\n",
    "#*******************************************************************************\n",
    "# %% codecell\n",
    "def steadyState(transitionMatrix):\n",
    "    ss_transitionMatrix = np.zeros(shape=(transitionMatrix.shape[0]))\n",
    "    ss_transitionMatrix = abs(pysal.spatial_dynamics.ergodic.\n",
    "                                 steady_state(transitionMatrix))\n",
    "    return ss_transitionMatrix\n",
    "\n",
    "\n",
    "def bandwidthPercentage(vectorMatrix):\n",
    "    percentageIncreament = (100 / vectorMatrix.shape[0])\n",
    "    percentageMatrix = np.zeros(shape=(vectorMatrix.shape[0]))\n",
    "    maxPercentage = 0\n",
    "    for j in range(vectorMatrix.shape[0]):\n",
    "        maxPercentage += percentageIncreament\n",
    "        percentageMatrix[j] = maxPercentage\n",
    "    return percentageMatrix\n",
    "\n",
    "\n",
    "def assymetricDistrib(percentageVector, arrayOfStatesPercentage, predictedStateIndex):\n",
    "    xAxisPoints = np.linspace(rayleigh.ppf(0.01), rayleigh.ppf(0.99), 338)\n",
    "    #number of overal datapoints must stay the same all the time\n",
    "    maxState = 338\n",
    "\n",
    "    inverseDistrib = max(rayleigh.pdf(xAxisPoints)) - rayleigh.pdf(xAxisPoints)\n",
    "    minState = np.argmin(inverseDistrib)\n",
    "\n",
    "    avgUnderUtilizedSum = 0\n",
    "    numberOfUnderUtilizedStates = minState - 0\n",
    "    for i in range(predictedStateIndex + 1, len(arrayOfStatesPercentage)):\n",
    "        underUtilizedPercentage = percentageVector[i] - percentageVector[predictedStateIndex]\n",
    "        index_On_UnderUtilized_Distribution = math.floor((underUtilizedPercentage *\n",
    "                                                          numberOfUnderUtilizedStates) / 100)\n",
    "        index_On_UnderUtilized_Distribution = minState - index_On_UnderUtilized_Distribution\n",
    "        avgUnderUtilizedSum += inverseDistrib[index_On_UnderUtilized_Distribution] * arrayOfStatesPercentage[i]\n",
    "\n",
    "    if predictedStateIndex != len(arrayOfStatesPercentage) - 1:\n",
    "        avgUnderUtilizedSum /= (len(arrayOfStatesPercentage) - (predictedStateIndex + 1))\n",
    "\n",
    "\n",
    "\n",
    "    avgOverUtilizedSum = 0\n",
    "    numberOfOverUtilizedStates = maxState - minState\n",
    "    for i in range(0, predictedStateIndex):\n",
    "        overUtilizedPercentage = percentageVector[predictedStateIndex] - percentageVector[i]\n",
    "        index_On_OverUtilized_Distribution = math.floor((overUtilizedPercentage *\n",
    "                                                         numberOfOverUtilizedStates) / 100)\n",
    "        index_On_OverUtilized_Distribution += minState\n",
    "        avgOverUtilizedSum += inverseDistrib[index_On_OverUtilized_Distribution] * arrayOfStatesPercentage[i]\n",
    "\n",
    "    if predictedStateIndex != 0:\n",
    "        avgOverUtilizedSum /= (predictedStateIndex)\n",
    "\n",
    "    avgPenalty = avgOverUtilizedSum + avgUnderUtilizedSum\n",
    "\n",
    "    # avgPenalty *= 1-(arrayOfStatesPercentage[predictedStateIndex])\n",
    "    return avgPenalty\n",
    "\n",
    "def statePenaltyValue(steadyStateVectorArray, percentageMatrix):\n",
    "    ssPenaltyValue = np.zeros(shape = (steadyStateVectorArray.shape[0]))\n",
    "    for j in range(steadyStateVectorArray.shape[0]):\n",
    "        ssPenaltyValue[j] = assymetricDistrib(percentageMatrix,\n",
    "                                              steadyStateVectorArray, j)\n",
    "\n",
    "    return ssPenaltyValue\n",
    "\n",
    "# statePenaltyNumpy = statePenaltyValue(steadyStateVectorArray, percentageMatrix)\n",
    "\n",
    "\n",
    "def neighbor_states_differences(statePenaltyNumpy):\n",
    "    neighbor_difference_array = np.zeros(shape = (statePenaltyNumpy.shape[0] - 1))\n",
    "    for j in range(statePenaltyNumpy.shape[0] - 1):\n",
    "        neighbor_difference_array[j] = abs(statePenaltyNumpy[j] -\n",
    "                                           statePenaltyNumpy[j + 1])\n",
    "\n",
    "    return neighbor_difference_array\n",
    "\n",
    "# neighbor_difference_array = nieghbor_states_differences(statePenaltyNumpy)\n",
    "\n",
    "def mergingStates(normalizedCuTrans, min_diff):\n",
    "    extra_Normalized_Transition_Matrix = np.zeros(shape = (normalizedCuTrans.shape[1] - 1,\n",
    "                                                           normalizedCuTrans.shape[1] - 1))\n",
    "\n",
    "    # print(\"sum of the min_diff: \", np.sum(normalizedCuTrans[min_diff]))\n",
    "    # print(\"sum of the min_diff + 1: \", np.sum(normalizedCuTrans[min_diff + 1]))\n",
    "    if np.sum(normalizedCuTrans[min_diff, :]) != 0 and np.sum(\n",
    "        normalizedCuTrans[min_diff + 1, :]) != 0:\n",
    "        normalizedCuTrans[min_diff, :] = normalizedCuTrans[min_diff, :] + normalizedCuTrans[min_diff + 1, :]\n",
    "        normalizedCuTrans[min_diff, :] = normalizedCuTrans[min_diff, :] /2\n",
    "    else:\n",
    "        normalizedCuTrans[min_diff, :] = normalizedCuTrans[min_diff, :] + normalizedCuTrans[min_diff + 1, :]\n",
    "\n",
    "    normalizedCuTrans[:, min_diff] = normalizedCuTrans[:, min_diff] + normalizedCuTrans[:, min_diff + 1]\n",
    "    extraNumpy = np.delete(normalizedCuTrans, min_diff + 1, 0)\n",
    "    extra_Normalized_Transition_Matrix = np.delete(extraNumpy, min_diff + 1, 1)\n",
    "    # print(\"sum of this row now is: \", np.sum(extra_Normalized_Transition_Matrix[min_diff]))\n",
    "\n",
    "    return extra_Normalized_Transition_Matrix\n",
    "\n",
    "def updatePercentage(percentageMatrix, min_diff):\n",
    "    newPercentageMatrix = np.zeros(shape = (percentageMatrix.shape[0] - 1))\n",
    "\n",
    "    newPercentageMatrix = np.delete(percentageMatrix, min_diff, 0)\n",
    "\n",
    "    return newPercentageMatrix\n",
    "# <codecell>\n",
    "#*******************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(normalizedTransitionMatrix, index):\n",
    "\n",
    "    overal_merged_states = []\n",
    "    overal_bandwidth_vector = []\n",
    "#     print(normalizedTransitionMatrix)\n",
    "#     print(np.sum(normalizedTransitionMatrix))\n",
    "#     print(normalizedTransitionMatrix.shape)\n",
    "\n",
    "    # for i in range(normalizedTransitionMatrix.sha - 1pe[0]):\n",
    "    for i in range(1):\n",
    "\n",
    "        # print(normalizedTransitionMatrix.shape)\n",
    "        percentageMatrix = bandwidthPercentage(normalizedTransitionMatrix)\n",
    "        # a percentage Matrix for each time interval\n",
    "\n",
    "        mergedStates = normalizedTransitionMatrix\n",
    "        #running for each time interval independently\n",
    "        while True:\n",
    "\n",
    "            steadyStateVector = steadyState(mergedStates)\n",
    "            # every time interval becomes a transition matrix vector\n",
    "\n",
    "            statePenaltyNumpy = statePenaltyValue(steadyStateVector, percentageMatrix)\n",
    "            # finding penalty for each state in the vector\n",
    "\n",
    "            neighbor_difference_array = neighbor_states_differences(statePenaltyNumpy)\n",
    "            # difference of neighbours' penlty\n",
    "\n",
    "            extra_diff_arr = neighbor_difference_array.copy()\n",
    "\n",
    "            constraint_counter = 0\n",
    "            while constraint_counter < extra_diff_arr.shape[0]:\n",
    "                min_index_in_penalty = np.argmin(extra_diff_arr)\n",
    "\n",
    "                if min_index_in_penalty != 0:\n",
    "                    if 25 < percentageMatrix[min_index_in_penalty + 1] - percentageMatrix[min_index_in_penalty - 1]:\n",
    "                        extra_diff_arr[min_index_in_penalty] = 100\n",
    "                        constraint_counter += 1\n",
    "                        continue\n",
    "\n",
    "                elif percentageMatrix[min_index_in_penalty + 1] > 25:\n",
    "                    extra_diff_arr[min_index_in_penalty] = 100\n",
    "                    constraint_counter += 1\n",
    "                    continue\n",
    "\n",
    "                mergedStates = mergingStates(mergedStates, min_index_in_penalty)\n",
    "                # merging states with closest difference w.r.t penalty\n",
    "\n",
    "                percentageMatrix = updatePercentage(percentageMatrix, min_index_in_penalty)\n",
    "                # updating bandwidth percentage after merging states\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "            if constraint_counter == extra_diff_arr.shape[0]:\n",
    "#                 print(\"the value of i is: \", index)\n",
    "#                 print(mergedStates)\n",
    "#                 print(percentageMatrix)\n",
    "                return mergedStates, percentageMatrix\n",
    "                break\n",
    "\n",
    "    return overal_merged_states, overal_bandwidth_vector\n",
    "\n",
    "def classifying(CU, boundaries):\n",
    "    occupiedBandwidth = (CU / 255) * 100\n",
    "    for i in range(boundaries.shape[0]):\n",
    "        if occupiedBandwidth <= boundaries[i]:\n",
    "            return i\n",
    "\n",
    "def preparingData(data, timeIndexValue, minuteSplit):\n",
    "    print(\"preparing training data\")\n",
    "    warnings.filterwarnings('always')\n",
    "    reg = \"l2\"\n",
    "    solvers = \"lbfgs\"\n",
    "    clf = LogisticRegression(penalty = reg, max_iter = 100000, random_state = 0,\n",
    "                             solver = solvers , multi_class = 'multinomial')\n",
    "    accuracyValue = 0\n",
    "    numOfElements = 0\n",
    "    f1scoreValue = 0\n",
    "    precisionValue = 0\n",
    "    recallValue = 0\n",
    "    prevRowTrain = np.inf\n",
    "    prevCU = np.inf\n",
    "\n",
    "    sampleIntervals = 6 #seconds\n",
    "    minuteSplit = 30 #minutes\n",
    "    numOfSamples = minuteSplit * 60 / sampleIntervals\n",
    "    # numberOfDays = len(numOfDays)\n",
    "    days = np.zeros(7)\n",
    "    numOfThirtyMinsPerDay = np.zeros(int((24 * 60) / minuteSplit)) #in this case 48\n",
    "    which6SecondsPerPeriod = np.zeros(int(minuteSplit * 60 / sampleIntervals)) #in this case 300\n",
    "    prevRowTrain = np.inf\n",
    "    prevCU = np.inf\n",
    "\n",
    "    XArraysForLearning = []\n",
    "    YArraysForLearning = []\n",
    "    XArraysForTesting = []\n",
    "    YArraysForTesting = []\n",
    "\n",
    "    x = timeIndexValue\n",
    "    iterPandas = data.loc[(data[\"timeIndex\"] == x)].copy()\n",
    "\n",
    "    iterPandas = iterPandas.sample(frac = 1)\n",
    "\n",
    "    trainingDataFrame = iterPandas.iloc[:int(np.floor(0.8 * len(iterPandas)))].copy()\n",
    "\n",
    "    trainingTransitionMatrix = processData.markovianTransitionMatrixDegree1(trainingDataFrame, 255, \"CU\")\n",
    "    normalizedTrainTrans = processData.normalizingTransMatrix(trainingTransitionMatrix)\n",
    "    normalizedTrainTransition = normalizedTrainTrans.copy()\n",
    "    transitionMatrix, boundaries = clustering(normalizedTrainTransition[-1], normalizedTrainTransition.shape[0])\n",
    "\n",
    "    trainingDataFrame[\"cuClass\"] = trainingDataFrame[\"CU\"].apply(lambda x: classifying(x, boundaries))\n",
    "\n",
    "    stackCounter = 0\n",
    "    prevCU = 0\n",
    "    for index, row in trainingDataFrame.iterrows():\n",
    "        lastCU = np.zeros(boundaries.shape[0])\n",
    "        lastCU[prevCU] = 1\n",
    "        which6SecondsPerPeriod[int(row[\"periodParticle\"])] = 1\n",
    "        XArray = lastCU\n",
    "        XArray = np.append(XArray, which6SecondsPerPeriod)\n",
    "\n",
    "        if stackCounter == 0:\n",
    "            XArraysForLearning = XArray\n",
    "        else:\n",
    "            XArraysForLearning = np.vstack((XArraysForLearning, XArray))\n",
    "\n",
    "        recentCU = np.zeros(boundaries.shape[0])\n",
    "        recentCU[row[\"cuClass\"]] = 1\n",
    "\n",
    "        if stackCounter == 0:\n",
    "            YArraysForLearning = recentCU\n",
    "            stackCounter += 1\n",
    "        else:\n",
    "            YArraysForLearning = np.vstack((YArraysForLearning, recentCU))\n",
    "\n",
    "        which6SecondsPerPeriod[int(row[\"periodParticle\"])] = 0\n",
    "        prevCU = row[\"cuClass\"]\n",
    "\n",
    "\n",
    "    weights = np.random.randn(XArraysForLearning.shape[1])\n",
    "    print(\"preparing testing data\")\n",
    "    #********************LR testing********************\n",
    "\n",
    "\n",
    "    testingDataFrame = iterPandas.iloc[int(np.floor(0.2 * len(iterPandas))):].copy()\n",
    "\n",
    "    testingDataFrame[\"cuClass\"] = testingDataFrame[\"CU\"].apply(lambda x: classifying(x, boundaries))\n",
    "\n",
    "\n",
    "    stackCounter = 0\n",
    "    prevCU = 0\n",
    "    for index, row in testingDataFrame.iterrows():\n",
    "\n",
    "        lastCU = np.zeros(boundaries.shape[0])\n",
    "        lastCU[prevCU] = 1\n",
    "        which6SecondsPerPeriod[int(row[\"periodParticle\"])] = 1\n",
    "        XArray = lastCU\n",
    "        XArray = np.append(XArray, which6SecondsPerPeriod)\n",
    "\n",
    "        if stackCounter == 0:\n",
    "            XArraysForTesting = XArray\n",
    "\n",
    "        else:\n",
    "            XArraysForTesting = np.vstack((XArraysForTesting, XArray))\n",
    "#         print(XArraysForTesting)\n",
    "\n",
    "        recentCU = np.zeros(boundaries.shape[0])\n",
    "        recentCU[row[\"cuClass\"]] = 1\n",
    "\n",
    "        if stackCounter == 0:\n",
    "            YArraysForTesting = recentCU\n",
    "            stackCounter += 1\n",
    "        else:\n",
    "            YArraysForTesting = np.vstack((YArraysForTesting, recentCU))\n",
    "#         print(YArraysForTesting)\n",
    "\n",
    "        which6SecondsPerPeriod[int(row[\"periodParticle\"])] = 0\n",
    "        prevCU = row[\"cuClass\"]\n",
    "\n",
    "    # print(XArraysForLearning.shape)\n",
    "    # print(YArraysForLearning.shape)\n",
    "\n",
    "    return XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries\n",
    "    \n",
    "def tensorFlowLossFunction(XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries):\n",
    "#     print(XArraysForLearning.shape)\n",
    "#     print(YArraysForLearning.shape)\n",
    "    print(\"making tensors\")\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    numOfEpochs = 200\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.placeholder(tf.float32, shape = (batch_size, XArraysForLearning.shape[1]))\n",
    "        y_ = tf.placeholder(tf.float32, shape = (batch_size, YArraysForLearning.shape[1]))\n",
    "        W = tf.Variable(tf.truncated_normal([XArraysForLearning.shape[1], YArraysForLearning.shape[1]]), name=\"weights\", dtype=tf.float32)\n",
    "        b = tf.Variable(tf.truncated_normal([YArraysForLearning.shape[1]]), dtype=tf.float32)\n",
    "\n",
    "        tf_test_dataset64 = tf.constant(XArraysForTesting)\n",
    "        tf_test_dataset = tf.cast(tf_test_dataset64, tf.float32)\n",
    "\n",
    "\n",
    "        beta = 0.05\n",
    "        logits = tf.matmul(x, W)\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        # train_prediction = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = logits)\n",
    "        test_prediction = tf.nn.softmax(tf.add(tf.matmul(tf_test_dataset, W),b))\n",
    "\n",
    "        # x = XArraysForLearning[0:(0 + batch_size), :]\n",
    "        # y_ = tf.Variable(YArraysForLearning[0:(0 + batch_size), :])\n",
    "\n",
    "        # loss = assymetricLossFunction(train_prediction, y_, boundaries)\n",
    "#         loss = assymetricLossFunction(train_prediction, y_, boundaries)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y_)\n",
    "        # regularizer = tf.nn.l2_loss(W)\n",
    "        # loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "        # loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = train_prediction, labels = y_)\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "        prevAcc = 0\n",
    "        earlyStoppingCounter = 0\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initializing learning\")\n",
    "\n",
    "        numberOfBatchIteration = int(XArraysForLearning.shape[0] / batch_size)\n",
    "        restOfData = XArraysForLearning.shape[0] % batch_size\n",
    "        if restOfData != 0:\n",
    "            numberOfBatchIteration += 1\n",
    "        for epoch in range(numOfEpochs):\n",
    "            accuracyValue = 0\n",
    "            lossValue = 0\n",
    "            totalBatch = 0\n",
    "            i = 0\n",
    "            randomize = np.arange(XArraysForLearning.shape[0])\n",
    "            np.random.shuffle(randomize)\n",
    "            XArraysForLearning = XArraysForLearning[randomize]\n",
    "            YArraysForLearning = YArraysForLearning[randomize]\n",
    "\n",
    "            for iteration in range(numberOfBatchIteration):\n",
    "                if (iteration == numberOfBatchIteration - 1) and restOfData != 0:\n",
    "                    break\n",
    "                    batch_data = XArraysForLearning[i:, :]\n",
    "                    batch_labels = YArraysForLearning[i:, :]\n",
    "\n",
    "                else:\n",
    "                    batch_data = XArraysForLearning[i:(i + batch_size), :]\n",
    "                    batch_labels = YArraysForLearning[i:(i + batch_size), :]\n",
    "\n",
    "                    i += batch_size\n",
    "\n",
    "\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "                _, predictions, l = session.run([optimizer, train_prediction, loss], feed_dict=feed_dict)\n",
    "\n",
    "                lossValue = (np.sum(l))\n",
    "                totalBatch += batch_size\n",
    "                # print(accuracy(batch_data, batch_labels))\n",
    "                accuracyValue += accuracy(predictions, batch_labels) * batch_size\n",
    "                # if accuracy(test_prediction.eval(), YArraysForTesting) < prevAcc and earlyStoppingCounter == 400:\n",
    "                #     print(prevAcc)\n",
    "                #     break\n",
    "                #\n",
    "                # elif accuracy(test_prediction.eval(), YArraysForTesting) < prevAcc:\n",
    "                #     earlyStoppingCounter += 1\n",
    "                #\n",
    "                # elif accuracy(test_prediction.eval(), YArraysForTesting) >= prevAcc:\n",
    "                #     earlyStoppingCounter = 0\n",
    "\n",
    "            # print(\"Minibatch step {0}\".format(step))\n",
    "            # print(\"training Acc is: {:.3f}\".format(accuracy(predictions,batch_labels)))\n",
    "            # prevAcc = accuracy(test_prediction.eval(), YArraysForTesting)\n",
    "#             print(\"for epoch and loss:\", epoch, np.mean(l))\n",
    "            # print(lossValue/totalBatch)\n",
    "#             print(accuracyValue/totalBatch)\n",
    "\n",
    "        # print(session.run(W))\n",
    "        print(\"\\nPenalty Value: {:.3f}\".format(assymetricPredictionScore(test_prediction.eval(), YArraysForTesting, boundaries)))\n",
    "        print(\"\\naccuracy Acc: {:.3f}\".format(accuracy(test_prediction.eval(), YArraysForTesting)))\n",
    "        return accuracy(test_prediction.eval(), YArraysForTesting), assymetricPredictionScore(test_prediction.eval(), YArraysForTesting, boundaries)\n",
    "\n",
    "\n",
    "def assymetricLossFunction(prediction, correctLable, boundaries):\n",
    "    sess = tf.Session()\n",
    "    xAxisPoints = np.linspace(rayleigh.ppf(0.01), rayleigh.ppf(0.99), 338)\n",
    "    #number of overal datapoints must stay the same all the time\n",
    "    maxState = 338\n",
    "    inverseDistrib = max(rayleigh.pdf(xAxisPoints)) - rayleigh.pdf(xAxisPoints)\n",
    "    inverseDistrib = tf.constant(inverseDistrib)\n",
    "    xAxisPoints -= xAxisPoints[np.argmin(inverseDistrib)]\n",
    "    minState = np.argmin(inverseDistrib)\n",
    "    numberOfOverUtilizedStates = maxState - minState\n",
    "    numberOfUnderUtilizedStates = minState\n",
    "    minState = tf.constant(minState, tf.float32)\n",
    "    numberOfOverUtilizedStates = tf.constant(numberOfOverUtilizedStates, tf.float32)\n",
    "    numberOfUnderUtilizedStates = tf.constant(numberOfUnderUtilizedStates, tf.float32)\n",
    "\n",
    "    underUtilVal = numberOfUnderUtilizedStates / 100\n",
    "    overUtilVal = numberOfOverUtilizedStates / 100\n",
    "\n",
    "\n",
    "    boundaries = tf.constant(boundaries, tf.float32)\n",
    "    correctLableIndex = tf.argmax(correctLable, 1)\n",
    "\n",
    "    diffPercentage = []\n",
    "    for index in range(correctLableIndex.shape[0]):\n",
    "        diffPercentage.append(boundaries[correctLableIndex[index]] - boundaries[:])\n",
    "\n",
    "    diffPercentage = tf.stack(diffPercentage)\n",
    "\n",
    "    penalties = []\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(diffPercentage.shape[0]):\n",
    "        for j in range(diffPercentage.shape[1]):\n",
    "            counter += 1\n",
    "\n",
    "            penalties.append(tf.cond(\n",
    "                    tf.greater(diffPercentage[i][j], 0),\n",
    "                    lambda: inverseDistrib[tf.dtypes.cast(minState + tf.math.floor\n",
    "                                                                   (tf.math.scalar_mul(diffPercentage[i][j],\n",
    "                                                                                       overUtilVal)), tf.int32)],\n",
    "                    lambda: inverseDistrib[tf.dtypes.cast(minState + tf.math.floor\n",
    "                                                                   (tf.math.scalar_mul\n",
    "                                                                    (diffPercentage[i][j], underUtilVal))\n",
    "                                                                   , tf.int32)]\n",
    "                    ))\n",
    "\n",
    "    penalties = tf.stack(penalties)\n",
    "    penalties = tf.dtypes.cast(penalties, tf.float32)\n",
    "    penalties = tf.reshape(penalties, diffPercentage.shape)\n",
    "    penalties = penalties / tf.norm(penalties)\n",
    "\n",
    "    # weights = tf.reduce_sum(penalties * (1-prediction), axis=1)\n",
    "    weights = (1 - penalties) * prediction\n",
    "    weights = weights / tf.norm(weights)\n",
    "    # print(correctLable)\n",
    "    # print(prediction)\n",
    "    # print(penalties)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels = correctLable, logits = weights)\n",
    "    # weighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels = penalties, logits = prediction)\n",
    "    # loss = tf.reduce_sum(weighted_losses)\n",
    "    # loss = tf.reduce_sum(penalties * prediction)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def assymetricPredictionScore(predictedLables, trueLables, boundaries):\n",
    "    xAxisPoints = np.linspace(rayleigh.ppf(0.01), rayleigh.ppf(0.99), 338)\n",
    "    #number of overal datapoints must stay the same all the time\n",
    "    maxState = 338\n",
    "\n",
    "    inverseDistrib = max(rayleigh.pdf(xAxisPoints)) - rayleigh.pdf(xAxisPoints)\n",
    "    minState = np.argmin(inverseDistrib)\n",
    "\n",
    "    underUtilizedSum = 0\n",
    "    overUtilizedSum = 0\n",
    "    numberOfUnderUtilizedStates = minState - 0\n",
    "    numberOfOverUtilizedStates = maxState - minState\n",
    "\n",
    "    xAxisPoints -= xAxisPoints[np.argmin(inverseDistrib)]\n",
    "\n",
    "    underUtilVal = numberOfUnderUtilizedStates / 100\n",
    "    overUtilVal = numberOfOverUtilizedStates / 100\n",
    "\n",
    "    correctLableIndex = np.argmax(trueLables, 1)\n",
    "    predictionIndex = np.argmax(predictedLables, 1)\n",
    "\n",
    "    diffPercentage = np.zeros(shape = (predictedLables.shape))\n",
    "\n",
    "    penalties = np.zeros(shape = (predictedLables.shape))\n",
    "\n",
    "    for index in range(predictedLables.shape[0]):\n",
    "        diffPercentage[index] = boundaries[correctLableIndex[index]] - boundaries[:]\n",
    "\n",
    "\n",
    "    for i in range(diffPercentage.shape[0]):\n",
    "        for j in range(diffPercentage.shape[1]):\n",
    "            if diffPercentage[i][j] > 0:\n",
    "                penalties[i][j] = inverseDistrib[minState + math.floor\n",
    "                                                  (diffPercentage[i][j] * overUtilVal)]\n",
    "            else:\n",
    "                penalties[i][j] = inverseDistrib[minState + math.floor\n",
    "                                                  (diffPercentage[i][j] * underUtilVal)]\n",
    "\n",
    "    sumOfPenalty = 0\n",
    "    for i in range(predictionIndex.shape[0]):\n",
    "        penalties[i] = penalties[i] / np.sum(penalties[i])\n",
    "        sumOfPenalty += penalties[i][predictionIndex[i]]\n",
    "\n",
    "    return sumOfPenalty\n",
    "\n",
    "def accuracy(predictedLables, trueLables):\n",
    "    import sys\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    correctLableIndex = np.argmax(trueLables, 1)\n",
    "    predictionIndex = np.argmax(predictedLables, 1)\n",
    "    acc = np.float64(np.sum(correctLableIndex == predictionIndex)/predictedLables.shape[0])\n",
    "    # wrongPred = np.where(predictionIndex != correctLableIndex)\n",
    "    # print(predictionIndex[wrongPred])\n",
    "    # print(correctLableIndex[wrongPred])\n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorFlowLossFunctionPenalty(XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries):\n",
    "#     print(XArraysForLearning.shape)\n",
    "#     print(YArraysForLearning.shape)\n",
    "    print(\"making tensors\")\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    numOfEpochs = 200\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.placeholder(tf.float32, shape = (batch_size, XArraysForLearning.shape[1]))\n",
    "        y_ = tf.placeholder(tf.float32, shape = (batch_size, YArraysForLearning.shape[1]))\n",
    "        W = tf.Variable(tf.truncated_normal([XArraysForLearning.shape[1], YArraysForLearning.shape[1]]), name=\"weights\", dtype=tf.float32)\n",
    "        b = tf.Variable(tf.truncated_normal([YArraysForLearning.shape[1]]), dtype=tf.float32)\n",
    "\n",
    "        tf_test_dataset64 = tf.constant(XArraysForTesting)\n",
    "        tf_test_dataset = tf.cast(tf_test_dataset64, tf.float32)\n",
    "\n",
    "\n",
    "        beta = 0.05\n",
    "        logits = tf.matmul(x, W)\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        # train_prediction = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_, logits = logits)\n",
    "        test_prediction = tf.nn.softmax(tf.add(tf.matmul(tf_test_dataset, W),b))\n",
    "\n",
    "        # x = XArraysForLearning[0:(0 + batch_size), :]\n",
    "        # y_ = tf.Variable(YArraysForLearning[0:(0 + batch_size), :])\n",
    "\n",
    "        # loss = assymetricLossFunction(train_prediction, y_, boundaries)\n",
    "        loss = assymetricLossFunction(train_prediction, y_, boundaries)\n",
    "#         loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y_)\n",
    "        # regularizer = tf.nn.l2_loss(W)\n",
    "        # loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "        # loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = train_prediction, labels = y_)\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "        prevAcc = 0\n",
    "        earlyStoppingCounter = 0\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initializing learning\")\n",
    "\n",
    "        numberOfBatchIteration = int(XArraysForLearning.shape[0] / batch_size)\n",
    "        restOfData = XArraysForLearning.shape[0] % batch_size\n",
    "        if restOfData != 0:\n",
    "            numberOfBatchIteration += 1\n",
    "        for epoch in range(numOfEpochs):\n",
    "            accuracyValue = 0\n",
    "            lossValue = 0\n",
    "            totalBatch = 0\n",
    "            i = 0\n",
    "            randomize = np.arange(XArraysForLearning.shape[0])\n",
    "            np.random.shuffle(randomize)\n",
    "            XArraysForLearning = XArraysForLearning[randomize]\n",
    "            YArraysForLearning = YArraysForLearning[randomize]\n",
    "\n",
    "            for iteration in range(numberOfBatchIteration):\n",
    "                if (iteration == numberOfBatchIteration - 1) and restOfData != 0:\n",
    "                    break\n",
    "                    batch_data = XArraysForLearning[i:, :]\n",
    "                    batch_labels = YArraysForLearning[i:, :]\n",
    "\n",
    "                else:\n",
    "                    batch_data = XArraysForLearning[i:(i + batch_size), :]\n",
    "                    batch_labels = YArraysForLearning[i:(i + batch_size), :]\n",
    "\n",
    "                    i += batch_size\n",
    "\n",
    "\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "                _, predictions, l = session.run([optimizer, train_prediction, loss], feed_dict=feed_dict)\n",
    "\n",
    "                lossValue = (np.sum(l))\n",
    "                totalBatch += batch_size\n",
    "                # print(accuracy(batch_data, batch_labels))\n",
    "                accuracyValue += accuracy(predictions, batch_labels) * batch_size\n",
    "                # if accuracy(test_prediction.eval(), YArraysForTesting) < prevAcc and earlyStoppingCounter == 400:\n",
    "                #     print(prevAcc)\n",
    "                #     break\n",
    "                #\n",
    "                # elif accuracy(test_prediction.eval(), YArraysForTesting) < prevAcc:\n",
    "                #     earlyStoppingCounter += 1\n",
    "                #\n",
    "                # elif accuracy(test_prediction.eval(), YArraysForTesting) >= prevAcc:\n",
    "                #     earlyStoppingCounter = 0\n",
    "\n",
    "            # print(\"Minibatch step {0}\".format(step))\n",
    "            # print(\"training Acc is: {:.3f}\".format(accuracy(predictions,batch_labels)))\n",
    "            # prevAcc = accuracy(test_prediction.eval(), YArraysForTesting)\n",
    "#             print(\"for epoch and loss:\", epoch, np.mean(l))\n",
    "            # print(lossValue/totalBatch)\n",
    "#             print(accuracyValue/totalBatch)\n",
    "\n",
    "        # print(session.run(W))\n",
    "        print(\"\\nPenalty Value: {:.3f}\".format(assymetricPredictionScore(test_prediction.eval(), YArraysForTesting, boundaries)))\n",
    "        print(\"\\naccuracy Acc: {:.3f}\".format(accuracy(test_prediction.eval(), YArraysForTesting)))\n",
    "        return accuracy(test_prediction.eval(), YArraysForTesting), assymetricPredictionScore(test_prediction.eval(), YArraysForTesting, boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index is:  0\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 303.569\n",
      "\n",
      "accuracy Acc: 0.914\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 299.545\n",
      "\n",
      "accuracy Acc: 0.914\n",
      "index is:  1\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 120.620\n",
      "\n",
      "accuracy Acc: 0.926\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 120.012\n",
      "\n",
      "accuracy Acc: 0.927\n",
      "index is:  2\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 237.702\n",
      "\n",
      "accuracy Acc: 0.905\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 236.799\n",
      "\n",
      "accuracy Acc: 0.905\n",
      "index is:  3\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 2088.787\n",
      "\n",
      "accuracy Acc: 0.350\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1696.584\n",
      "\n",
      "accuracy Acc: 0.421\n",
      "index is:  4\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1768.379\n",
      "\n",
      "accuracy Acc: 0.370\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 204.445\n",
      "\n",
      "accuracy Acc: 0.672\n",
      "index is:  5\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 73.380\n",
      "\n",
      "accuracy Acc: 0.938\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 78.630\n",
      "\n",
      "accuracy Acc: 0.938\n",
      "index is:  6\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 333.522\n",
      "\n",
      "accuracy Acc: 0.925\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 332.461\n",
      "\n",
      "accuracy Acc: 0.925\n",
      "index is:  7\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 272.311\n",
      "\n",
      "accuracy Acc: 0.872\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 268.564\n",
      "\n",
      "accuracy Acc: 0.867\n",
      "index is:  8\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 539.690\n",
      "\n",
      "accuracy Acc: 0.919\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 536.117\n",
      "\n",
      "accuracy Acc: 0.919\n",
      "index is:  9\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 309.389\n",
      "\n",
      "accuracy Acc: 0.918\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 308.181\n",
      "\n",
      "accuracy Acc: 0.919\n",
      "index is:  10\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 483.760\n",
      "\n",
      "accuracy Acc: 0.349\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 482.082\n",
      "\n",
      "accuracy Acc: 0.348\n",
      "index is:  11\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 299.344\n",
      "\n",
      "accuracy Acc: 0.901\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 297.052\n",
      "\n",
      "accuracy Acc: 0.901\n",
      "index is:  12\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 2442.097\n",
      "\n",
      "accuracy Acc: 0.167\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 169.276\n",
      "\n",
      "accuracy Acc: 0.783\n",
      "index is:  13\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 232.324\n",
      "\n",
      "accuracy Acc: 0.707\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 266.784\n",
      "\n",
      "accuracy Acc: 0.700\n",
      "index is:  14\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 828.879\n",
      "\n",
      "accuracy Acc: 0.518\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 865.009\n",
      "\n",
      "accuracy Acc: 0.563\n",
      "index is:  15\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 385.564\n",
      "\n",
      "accuracy Acc: 0.654\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 391.250\n",
      "\n",
      "accuracy Acc: 0.653\n",
      "index is:  16\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 380.844\n",
      "\n",
      "accuracy Acc: 0.341\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1329.185\n",
      "\n",
      "accuracy Acc: 0.619\n",
      "index is:  17\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 511.968\n",
      "\n",
      "accuracy Acc: 0.311\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1250.073\n",
      "\n",
      "accuracy Acc: 0.685\n",
      "index is:  18\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 370.351\n",
      "\n",
      "accuracy Acc: 0.856\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 366.693\n",
      "\n",
      "accuracy Acc: 0.856\n",
      "index is:  19\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 3368.674\n",
      "\n",
      "accuracy Acc: 0.501\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1029.645\n",
      "\n",
      "accuracy Acc: 0.498\n",
      "index is:  20\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 280.552\n",
      "\n",
      "accuracy Acc: 0.851\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 275.971\n",
      "\n",
      "accuracy Acc: 0.858\n",
      "index is:  21\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 887.454\n",
      "\n",
      "accuracy Acc: 0.759\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 889.052\n",
      "\n",
      "accuracy Acc: 0.759\n",
      "index is:  22\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 876.300\n",
      "\n",
      "accuracy Acc: 0.739\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 887.514\n",
      "\n",
      "accuracy Acc: 0.742\n",
      "index is:  23\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1092.812\n",
      "\n",
      "accuracy Acc: 0.521\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 764.584\n",
      "\n",
      "accuracy Acc: 0.527\n",
      "index is:  24\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 742.271\n",
      "\n",
      "accuracy Acc: 0.762\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 756.126\n",
      "\n",
      "accuracy Acc: 0.761\n",
      "index is:  25\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 580.542\n",
      "\n",
      "accuracy Acc: 0.783\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 579.352\n",
      "\n",
      "accuracy Acc: 0.782\n",
      "index is:  26\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 360.762\n",
      "\n",
      "accuracy Acc: 0.165\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 460.392\n",
      "\n",
      "accuracy Acc: 0.786\n",
      "index is:  27\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 762.958\n",
      "\n",
      "accuracy Acc: 0.599\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 890.948\n",
      "\n",
      "accuracy Acc: 0.719\n",
      "index is:  28\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 868.918\n",
      "\n",
      "accuracy Acc: 0.779\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 869.268\n",
      "\n",
      "accuracy Acc: 0.779\n",
      "index is:  29\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 512.656\n",
      "\n",
      "accuracy Acc: 0.338\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1289.198\n",
      "\n",
      "accuracy Acc: 0.600\n",
      "index is:  30\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 852.278\n",
      "\n",
      "accuracy Acc: 0.279\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1850.580\n",
      "\n",
      "accuracy Acc: 0.521\n",
      "index is:  31\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 731.389\n",
      "\n",
      "accuracy Acc: 0.274\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 939.928\n",
      "\n",
      "accuracy Acc: 0.747\n",
      "index is:  32\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1926.471\n",
      "\n",
      "accuracy Acc: 0.573\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1915.626\n",
      "\n",
      "accuracy Acc: 0.573\n",
      "index is:  33\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 2277.977\n",
      "\n",
      "accuracy Acc: 0.536\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 2277.155\n",
      "\n",
      "accuracy Acc: 0.536\n",
      "index is:  34\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1262.243\n",
      "\n",
      "accuracy Acc: 0.514\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 567.556\n",
      "\n",
      "accuracy Acc: 0.371\n",
      "index is:  35\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 829.049\n",
      "\n",
      "accuracy Acc: 0.274\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1201.721\n",
      "\n",
      "accuracy Acc: 0.519\n",
      "index is:  36\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Penalty Value: 1397.713\n",
      "\n",
      "accuracy Acc: 0.660\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1392.056\n",
      "\n",
      "accuracy Acc: 0.659\n",
      "index is:  37\n",
      "preparing training data\n",
      "preparing testing data\n",
      "making tensors\n",
      "Initializing learning\n",
      "\n",
      "Penalty Value: 1818.935\n",
      "\n",
      "accuracy Acc: 0.725\n",
      "making tensors\n",
      "Initializing learning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d83080344961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFlowLossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXArraysForLearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYArraysForLearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXArraysForTesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYArraysForTesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time index \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" with accuracy \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" with penalty \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFlowLossFunctionPenalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXArraysForLearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYArraysForLearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXArraysForTesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYArraysForTesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpenalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time index \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" with accuracy \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" with penalty \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0d3eecfad24c>\u001b[0m in \u001b[0;36mtensorFlowLossFunctionPenalty\u001b[0;34m(XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mlossValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "normal = open(\"mergingPenaltyStates_normalLoss_30min.txt\",\"w+\")\n",
    "penalized = open(\"mergingPenaltyStates_penalizedLoss_30min.txt\",\"w+\")\n",
    "\n",
    "for i in range(max(data[\"timeIndex\"]) + 1):\n",
    "    print(\"index is: \", i)\n",
    "    XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries = preparingData(data, i, 30)\n",
    "    acc, penalty = tensorFlowLossFunction(XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries)\n",
    "    normal.write(\"time index \" + str(i) + \" with accuracy \" + str(acc) + \" with penalty \" + str(penalty))\n",
    "    acc, penalty = tensorFlowLossFunctionPenalty(XArraysForLearning, YArraysForLearning, XArraysForTesting, YArraysForTesting, boundaries)\n",
    "    penalized.write(\"time index \" + str(i) + \" with accuracy \" + str(acc) + \" with penalty \" + str(penalty))\n",
    "\n",
    "normal.close()\n",
    "penalized.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
